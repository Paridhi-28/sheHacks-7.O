{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locationtagger\n",
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "import os\n",
    "# from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import certifi; \n",
    "\n",
    "\n",
    "# essential entity models downloads\n",
    "nltk.downloader.download('maxent_ne_chunker')\n",
    "nltk.downloader.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "nltk.downloader.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-12-22'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datetime.today() - timedelta(days=30)).strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTweetData(tweet):\n",
    "    try:\n",
    "        tweetTime = tweet.find_element(\n",
    "            By.XPATH, \".//time\").get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    try:\n",
    "        # This only gets one image, what if there are more?\n",
    "        tweetImage = tweet.find_element(\n",
    "            By.XPATH, \".//div[1]/div[1]//div[2]/div[2]//img\").get_attribute(\"src\")\n",
    "    except NoSuchElementException:\n",
    "        # print(\"no image\")\n",
    "        tweetImage = \"No Image\"\n",
    "    tweetText = tweet.find_element(\n",
    "        By.XPATH, \".//div[1]/div[1]/div[2]/div[2]/div[2]\").text\n",
    "\n",
    "    tweetInfo = [tweetText, tweetTime, tweetImage]\n",
    "    return tweetInfo\n",
    "\n",
    "\n",
    "def ScraperMain():\n",
    "    twitterUsername = os.getenv('TW_USERNAME')\n",
    "    twitterPassword = os.getenv('TW_PASSWORD')\n",
    "    \n",
    "\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "    # driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get('https://www.twitter.com/login')\n",
    "    sleep(3)\n",
    "    # Finding and inputing username\n",
    "    username = driver.find_element(\n",
    "        By.XPATH, \"//body/div[@id='react-root']/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[1]/div[5]/label[1]/div[1]/div[2]/div[1]/input[1]\")\n",
    "    username.send_keys(twitterUsername)\n",
    "    username.send_keys(Keys.RETURN)\n",
    "    sleep(1)\n",
    "    # Finding and inputing Password\n",
    "    # mypassword = getpass()\n",
    "    password = driver.find_element(\n",
    "        By.XPATH, \"//body/div[@id='react-root']/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[1]/div[3]/div[1]/label[1]/div[1]/div[2]/div[1]/input[1]\")\n",
    "    password.send_keys(twitterPassword)\n",
    "    password.send_keys(Keys.RETURN)\n",
    "    sleep(5)\n",
    "\n",
    "    # Selecting and searching required police section\n",
    "    search = driver.find_element(\n",
    "        By.XPATH, \"//body/div[@id='react-root']/div[1]/div[1]/div[2]/main[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/form[1]/div[1]/div[1]/div[1]/div[1]/label[1]/div[2]/div[1]/input[1]\")\n",
    "    search.send_keys('@TPSOperations')\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    sleep(2)\n",
    "\n",
    "    # Click required page\n",
    "    SearchQuery = driver.find_element(\n",
    "        By.XPATH, \"//span[contains(text(),'Toronto Police Operations')]\")\n",
    "    SearchQuery.click()\n",
    "\n",
    "    # Storing Tweet data in list\n",
    "    tweetData = []\n",
    "    tweetIds = set()\n",
    "    lastPos = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    scrolling = True\n",
    "    maxDate = False\n",
    "    stopDate = str((datetime.today() - timedelta(days=30)).strftime('%Y-%m-%d'))\n",
    "\n",
    "    while (scrolling and not (maxDate)):\n",
    "        # Find Tweets\n",
    "        TPOPageTweets = driver.find_elements(\n",
    "            By.XPATH, '//article[@data-testid=\"tweet\"]')\n",
    "        sleep(1)\n",
    "        for tweet in TPOPageTweets[-100:]:\n",
    "            currentTweetInfo = getTweetData(tweet)\n",
    "            if currentTweetInfo:\n",
    "                tweetId = \"\".join(currentTweetInfo)\n",
    "                if tweet not in tweetIds and currentTweetInfo[0] != '' and \":\" in currentTweetInfo[0] and 'Good night' not in currentTweetInfo[0] and 'Good afternoon' not in currentTweetInfo[0] and 'Good morning' not in currentTweetInfo[0] and 'Good Night' not in currentTweetInfo[0]:\n",
    "                    tweetIds.add(tweet)\n",
    "                    currentTweetInfo.append(tweetId)\n",
    "                    tweetData.append(currentTweetInfo)\n",
    "                    if stopDate in currentTweetInfo[1]:\n",
    "                        maxDate = True\n",
    "                        break\n",
    "\n",
    "        scrollAttempt = 0\n",
    "        while True:\n",
    "            driver.execute_script('window.scrollBy(0,3000);')\n",
    "            sleep(2)\n",
    "            currPos = driver.execute_script(\"return window.pageYOffset;\")\n",
    "            if maxDate:\n",
    "                break\n",
    "            if lastPos == currPos:\n",
    "                scrollAttempt += 1\n",
    "                if scrollAttempt >= 3:\n",
    "                    scrolling = False\n",
    "                    break\n",
    "                else:\n",
    "                    sleep(2)\n",
    "            else:\n",
    "                lastPos = currPos\n",
    "                break\n",
    "    print(\"Tweets Scraped\")\n",
    "    return tweetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortData(scrapedTweet):\n",
    "    # List: catagory (Missing...), Status(Update, Located)\n",
    "    computedData = []\n",
    "    counter = 0\n",
    "    for i in range(len(scrapedTweet)):\n",
    "        dataObject = {}\n",
    "        if (scrapedTweet[i][0].find('\\n') != -1):\n",
    "            # Getting Status:\n",
    "            dataObject[\"Status\"] = scrapedTweet[i][0].splitlines()[0].split(\":\")[0]\n",
    "            # Getting Updates (?LOCATED)...\n",
    "            try:\n",
    "                dataObject[\"Updates\"] = scrapedTweet[i][0].splitlines()[0].split(\":\")[1]\n",
    "            except IndexError:\n",
    "                dataObject[\"Updates\"] = \"\"\n",
    "            # currentLocation could be the currentIdentity\n",
    "            currentLocation = scrapedTweet[i][0].splitlines()[1].split(\", \")\n",
    "            # Checking whether we have a Location or an identity\n",
    "            if any(char.isdigit() for char in currentLocation):\n",
    "                dataObject[\"Name\"] = currentLocation[0]\n",
    "                if currentLocation[1].isnumeric():\n",
    "                    dataObject[\"Age\"] = currentLocation[1]\n",
    "                else:\n",
    "                    dataObject[\"Age\"] = \"\"\n",
    "                description = scrapedTweet[i][0].splitlines()\n",
    "                del description[0:2]\n",
    "                #Extracting Location from Description\n",
    "                lists = scrapedTweet[i][0].splitlines()\n",
    "                del lists[0:2]\n",
    "                try:\n",
    "                    partOfDescription = lists[0] + \" \" + lists[1]\n",
    "                except IndexError:\n",
    "                    partOfDescription = scrapedTweet[i][0]                   \n",
    "                dataObject[\"Location\"] = locationtagger.find_locations(text = partOfDescription).other\n",
    "            else:\n",
    "                try:\n",
    "                    dataObject[\"Location\"] = scrapedTweet[i][0].splitlines()[1]\n",
    "                except IndexError:\n",
    "                    dataObject[\"Location\"] = \"\"\n",
    "                description = scrapedTweet[i][0].splitlines()\n",
    "                del description[0:1]\n",
    "            if dataObject[\"Location\"] is not None:\n",
    "                locationcheck = dataObject[\"Location\"]\n",
    "                if type(dataObject[\"Location\"]) == list and bool(dataObject[\"Location\"]):\n",
    "                    res = max(locationcheck, key = len)\n",
    "                    locationcheck = res\n",
    "                if \"and\" in locationcheck:\n",
    "                    locationcheck = locationcheck.replace(\"and\", \"\")\n",
    "                if \"&\" in locationcheck:\n",
    "                    locationcheck = locationcheck.replace(\"&\", \"\")\n",
    "                geolocator = Nominatim(user_agent=\"Twitter_Scraper\")\n",
    "                geocode = lambda query: geolocator.geocode(\"%s, Toronto ON\" % query)\n",
    "                geocode2 = RateLimiter(geocode, min_delay_seconds=0.001)\n",
    "                sleep(1)\n",
    "                locationcor = geocode2(locationcheck)\n",
    "                if locationcor is not None:\n",
    "                    locationcor2 = [locationcor.latitude, locationcor.longitude]\n",
    "                    dataObject[\"LocationGoeCode\"] = locationcor2\n",
    "                else:\n",
    "                    dataObject[\"LocationGoeCode\"] = []\n",
    "                    \n",
    "            else:\n",
    "                dataObject[\"LocationGoeCode\"] = [\"No Location Provided\"]\n",
    "            dataObject[\"TweetedTime\"] = scrapedTweet[i][1]\n",
    "            dataObject[\"ImageUrl\"] = scrapedTweet[i][2]\n",
    "            dataObject[\"Description\"] = description\n",
    "            computedData.append(dataObject)\n",
    "            counter = counter + 1\n",
    "            print(counter)\n",
    "            print(dataObject['Location'])\n",
    "            print(dataObject['LocationGoeCode'])\n",
    "        else:\n",
    "            print(\"Tweet Skipped: \", scrapedTweet[i])\n",
    "    print(\"Tweets Sorted\")\n",
    "    return(computedData)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def addToDB(computedData):\n",
    "    MONGODB_PASS = os.getenv('MONGODB_PASS')\n",
    "\n",
    "    CONNECTION_STRING = \"mongodb+srv://OCMB1:\" + MONGODB_PASS + \\\n",
    "        \"@cluster0.e5wloiz.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "    client = MongoClient(CONNECTION_STRING, tlsCAFile=certifi.where())\n",
    "\n",
    "    if 'Toronto_Police_Crime_Report' in client.list_database_names():\n",
    "        client.drop_database('Toronto_Police_Crime_Report')\n",
    "\n",
    "    db = client['Toronto_Police_Crime_Report']\n",
    "    collection_name = db[\"tweetsData\"]\n",
    "    res = collection_name.insert_many(computedData)\n",
    "    print(\"Added To DataBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Scraped\n"
     ]
    }
   ],
   "source": [
    "tweets = ScraperMain()\n",
    "# sortedData = sortData(tweets)\n",
    "# addToDB(sortedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedData = sortData(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added To DataBase\n"
     ]
    }
   ],
   "source": [
    "addToDB(sortedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"Twitter\")\n",
    "geocode = lambda query: geolocator.geocode(\"%s, Toronto ON\" % query)\n",
    "location = geocode(\"Victoria Park Van Horne\")\n",
    "location2 = [location.latitude, location.longitude]\n",
    "location2\n",
    "# print((location.latitude, location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace YOUR_API_KEY with your actual API key. Sign up and get an API key on https://www.geoapify.com/ \n",
    "API_KEY = \"e904244da7514e13a457430791adcf16\"\n",
    "\n",
    "# Define the address to geocode\n",
    "address = \"Jane St Wilson Ave area, Toronto, CA\"\n",
    "\n",
    "# Build the API URL\n",
    "url = f\"https://api.geoapify.com/v1/geocode/search?text={address}&limit=1&apiKey={API_KEY}\"\n",
    "\n",
    "# Send the API request and get the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON data from the response\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the first result from the data\n",
    "    result = data[\"features\"][0]\n",
    "\n",
    "    # Extract the latitude and longitude of the result\n",
    "    latitude = result[\"geometry\"][\"coordinates\"][1]\n",
    "    longitude = result[\"geometry\"][\"coordinates\"][0]\n",
    "\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
